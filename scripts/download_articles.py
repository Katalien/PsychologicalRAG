import json
import os
import requests
from bs4 import BeautifulSoup
import html2text
import time
import re
from urllib.parse import urlparse
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ArticleDownloader:
    def __init__(self, base_json_path, articles_dir):
        self.base_json_path = base_json_path
        self.articles_dir = articles_dir
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è html2text –¥–ª—è —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        self.html_converter = html2text.HTML2Text()
        self.html_converter.ignore_links = False
        self.html_converter.ignore_images = True
        self.html_converter.body_width = 0
        
        # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –¥–ª—è —Å—Ç–∞—Ç–µ–π, –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
        os.makedirs(self.articles_dir, exist_ok=True)

    def clean_text(self, text):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –∏ —Å–∏–º–≤–æ–ª–æ–≤"""
        text = re.sub(r'\n\s*\n', '\n\n', text)  # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        text = re.sub(r'[ \t]+', ' ', text)      # –£–¥–∞–ª—è–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        return text.strip()

    def extract_main_content(self, soup, url):
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏, —É–±–∏—Ä–∞—è –±–æ–∫–æ–≤—ã–µ –ø–∞–Ω–µ–ª–∏, –º–µ–Ω—é –∏ —Ç.–¥.
        –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Å–∞–π—Ç–æ–≤.
        """
        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup.find_all(['script', 'style', 'nav', 'header', 'footer', 'aside']):
            element.decompose()

        # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –ø–æ–∏—Å–∫–∞ –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞)
        content_selectors = [
            'article',
            '[class*="article"]',
            '[class*="content"]',
            '[class*="post"]',
            '[class*="blog"]',
            'main',
            '.entry-content',
            '.post-content',
            '.article-content',
            '#content',
            '.content',
            'div[role="main"]'
        ]

        main_content = None
        
        for selector in content_selectors:
            main_content = soup.select_one(selector)
            if main_content and len(main_content.get_text(strip=True)) > 200:
                break

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º, –∏—Å–ø–æ–ª—å–∑—É–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
        if not main_content:
            # –ò—â–µ–º div —Å –Ω–∞–∏–±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–µ–∫—Å—Ç–∞
            all_divs = soup.find_all('div')
            if all_divs:
                main_content = max(all_divs, key=lambda x: len(x.get_text(strip=True)))
        
        return main_content

    def download_article(self, url, article_id, category):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –∏ –æ—á–∏—â–∞–µ—Ç —Å—Ç–∞—Ç—å—é –ø–æ URL"""
        try:
            logger.info(f"–°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç–∞—Ç—å—é {article_id} –∏–∑ {url}")
            
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            main_content = self.extract_main_content(soup, url)
            
            if not main_content:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Å—Ç–∞—Ç—å–∏ {article_id}")
                return None
            
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç
            html_content = str(main_content)
            text_content = self.html_converter.handle(html_content)
            clean_content = self.clean_text(text_content)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–Ω—Ç–µ–Ω—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –±–æ–ª—å—à–æ–π
            if len(clean_content) < 100:
                logger.warning(f"–°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è —Å—Ç–∞—Ç—å–∏ {article_id}")
                return None
            
            return clean_content
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏ {article_id}: {e}")
            return None

    def save_article(self, content, article_id, category):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Ç–∞—Ç—å—é –≤ —Ñ–∞–π–ª"""
        filename = f"{category}_{article_id}.txt"
        filepath = os.path.join(self.articles_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(content)
            logger.info(f"–°—Ç–∞—Ç—å—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ —Å—Ç–∞—Ç—å–∏ {filename}: {e}")
            return False

    def download_all_articles(self, delay=2):
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ base_dataset.json"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –±–∞–∑–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        with open(self.base_json_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        
        success_count = 0
        fail_count = 0
        skipped_count = 0
        
        for item in dataset['rows']:
            article_id = item['id']
            category = item['category']
            url = item['link']
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å–∫–∞—á–∞–Ω–∞ –ª–∏ —É–∂–µ —Å—Ç–∞—Ç—å—è
            filename = f"{category}_{article_id}.txt"
            filepath = os.path.join(self.articles_dir, filename)
            
            if os.path.exists(filepath):
                logger.info(f"–°—Ç–∞—Ç—å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º: {filename}")
                skipped_count += 1
                continue
            
            # –°–∫–∞—á–∏–≤–∞–µ–º —Å—Ç–∞—Ç—å—é
            content = self.download_article(url, article_id, category)
            
            if content:
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å—Ç–∞—Ç—å—é
                if self.save_article(content, article_id, category):
                    success_count += 1
                else:
                    fail_count += 1
            else:
                fail_count += 1
            
            # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            time.sleep(delay)
        
        # –í—ã–≤–æ–¥–∏–º –∏—Ç–æ–≥–∏
        logger.info(f"\nüìä –ò–¢–û–ì–ò –°–ö–ê–ß–ò–í–ê–ù–ò–Ø:")
        logger.info(f"   –£—Å–ø–µ—à–Ω–æ: {success_count}")
        logger.info(f"   –ü—Ä–æ–ø—É—â–µ–Ω–æ: {skipped_count}")
        logger.info(f"   –û—à–∏–±–∫–∏: {fail_count}")
        logger.info(f"   –í—Å–µ–≥–æ: {len(dataset['rows'])}")

    def check_download_status(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ç–∞—Ç—É—Å —Å–∫–∞—á–∏–≤–∞–Ω–∏—è —Å—Ç–∞—Ç–µ–π"""
        with open(self.base_json_path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)
        
        downloaded = 0
        missing = 0
        
        print("\nüîç –°–¢–ê–¢–£–° –°–ö–ê–ß–ò–í–ê–ù–ò–Ø –°–¢–ê–¢–ï–ô:")
        for item in dataset['rows']:
            article_id = item['id']
            category = item['category']
            filename = f"{category}_{article_id}.txt"
            filepath = os.path.join(self.articles_dir, filename)
            
            if os.path.exists(filepath):
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞
                file_size = os.path.getsize(filepath)
                status = "‚úÖ" if file_size > 100 else "‚ö†Ô∏è (–º–∞–ª–µ–Ω—å–∫–∏–π)"
                print(f"   {status} {filename} ({file_size} –±–∞–π—Ç)")
                downloaded += 1
            else:
                print(f"   ‚ùå {filename}")
                missing += 1
        
        print(f"\n   –°–∫–∞—á–∞–Ω–æ: {downloaded}/{len(dataset['rows'])}")
        print(f"   –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç: {missing}")

def main():
    # –ü—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º
    BASE_JSON_PATH = "../data/base_dataset.json"
    ARTICLES_DIR = "../data/articles"
    
    # –°–æ–∑–¥–∞–µ–º –∑–∞–≥—Ä—É–∑—á–∏–∫
    downloader = ArticleDownloader(BASE_JSON_PATH, ARTICLES_DIR)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ç–µ–∫—É—â–∏–π —Å—Ç–∞—Ç—É—Å
    downloader.check_download_status()
    
    # –°–ø—Ä–∞—à–∏–≤–∞–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    choice = input("\n–•–æ—Ç–∏—Ç–µ —Å–∫–∞—á–∞—Ç—å –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–∞—Ç—å–∏? (y/n): ")
    if choice.lower() == 'y':
        # –°–∫–∞—á–∏–≤–∞–µ–º –≤—Å–µ —Å—Ç–∞—Ç—å–∏
        downloader.download_all_articles(delay=1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ 1 —Å–µ–∫—É–Ω–¥–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
        
        # –°–Ω–æ–≤–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å
        downloader.check_download_status()

if __name__ == "__main__":
    main()